
---
title: "Math 575 Final Exam"
output:
  pdf_document: default
  html_document:
    df_print: paged
  html_notebook: default
---



## Problem 1

Note: This problem requires the use of the data set problem1.Rdata.

Write your own function to estimate the probability density of the data in the vector "problem_1_data" in Rdata file through kernel density estimation. Use a Gaussian kernel and the three different "rules of thumb" for the optimal value of the bandwidth parameter $h$ (see p. 298). Plot your density estimates for the three cases. Which of those do you think produces the "best" fit? 

Do the same, but for a non-Gaussian kernel of your choice. Don't forget to use the bandwitdh scaling parameter when switching your bandwitch parameters from Gaussian to this kernel (see p. 299). 

```{r problem1}
load("C:/Users/Owner/Downloads/problem1.Rdata")
length(problem_1_data)
head(problem_1_data)

S=sd(problem_1_data)
IQR=IQR(problem_1_data)
S
IQR/1.34


#kernel density estimation
#our_density_func=function(input_data,bandwidth){
gauss_kernel <- function(x){
  return(exp(-x^2/2)/(sqrt(2*pi)))
}
bandwidth1=(4/3)^(1/5)*sd(problem_1_data)/length(problem_1_data)^(1/5)
bandwidth3=(4/3)^(1/5)*(IQR/1.34)/length(problem_1_data)^(1/5)
bandwidth2=.9*(IQR/1.34)/length(problem_1_data)^(1/5)


our_density_func=function(input_data, bandwidth){
  output_func = function(x){
    dens_out <- 0
    for(i in 1:length(input_data)){
      input_to_kernel <- (x-input_data[i])/bandwidth
      dens_out <- dens_out + gauss_kernel(input_to_kernel)
    }
    return(dens_out / (bandwidth *length(input_data)))
  }
  
  return(output_func)
}

#}

hist(problem_1_data, breaks=60)
x_range=seq(0,13,by=.01)
density_function1=our_density_func(problem_1_data, bandwidth=bandwidth1)
plot(x_range,density_function1(x_range), type="l", ylim=c(0,.25))

density_function2=our_density_func(problem_1_data, bandwidth=bandwidth2)
plot(x_range,density_function2(x_range), type="l", ylim=c(0,.25))

density_function3=our_density_func(problem_1_data, bandwidth=bandwidth3)
plot(x_range,density_function3(x_range), type="l", ylim=c(0,.25))



hist(problem_1_data, breaks=60,prob=TRUE, main="Histogram of Data")
lines(x_range,density_function1(x_range), col="orange")
lines(x_range,density_function2(x_range), col="blue")
lines(x_range,density_function3(x_range), col="red")

```

**I'm going to have to go with the blue ($bandwidth=.9*(IQR/1.34)/length(problem_1_data)^(1/5)$) as having the "best" fit because of the tall bins close to x=0. They're all very close.**

Do the same, but for a non-Gaussian kernel of your choice. Don't forget to use the bandwitdh scaling parameter when switching your bandwitch parameters from Gaussian to this kernel (see p. 299). 
```{r}

biweight_kernel <- function(x){
  return(exp(-x^2/2)/(sqrt(2*pi)))
}
bandwidth1=(4/3)^(1/5)*sd(problem_1_data)/length(problem_1_data)^(1/5)
bandwidth3=(4/3)^(1/5)*(IQR/1.34)/length(problem_1_data)^(1/5)
bandwidth2=.9*(IQR/1.34)/length(problem_1_data)^(1/5)


our_density_func_new=function(input_data, bandwidth){
  output_func = function(x){
    dens_out <- 0
    for(i in 1:length(input_data)){
      input_to_kernel <- (x-input_data[i])/bandwidth
      dens_out <- dens_out + biweight_kernel(input_to_kernel)
    }
    return(dens_out / (bandwidth *length(input_data)))
  }
  
  return(output_func)
}


new_bandwidth_1=bandwidth1*(1/sqrt(1/7))
new_bandwidth_2=bandwidth2*(1/sqrt(1/7))
new_bandwidth_3=bandwidth3*(1/sqrt(1/7))

x_range=seq(0,13,by=.01)
density_function1_new=our_density_func_new(problem_1_data, bandwidth=new_bandwidth_1)
plot(x_range,density_function1_new(x_range), type="l", ylim=c(0,.25))

density_function2_new=our_density_func_new(problem_1_data, bandwidth=new_bandwidth_2)
plot(x_range,density_function2_new(x_range), type="l", ylim=c(0,.25))

density_function3_new=our_density_func_new(problem_1_data, bandwidth=new_bandwidth_3)
plot(x_range,density_function3_new(x_range), type="l", ylim=c(0,.25))



hist(problem_1_data, breaks=40,prob=TRUE, main="Histogram of Problem 1 Data")
lines(x_range,density_function1_new(x_range), col="orange")
lines(x_range,density_function2_new(x_range), col="blue")
lines(x_range,density_function3_new(x_range), col="red")


```



## Problem 2

With same data set as in Problem 1, create and apply your own kernel density estimator again, except this time use the reflection technique and plot your estimate. 
Use Silverman's rule of thumb for the bandwidth parameter. In your opinion, did you get a better estimate from what you created in Problem 1 or Problem 2? Should you compute the optimal bandwidth before or after the "reflecton?"

```{r problem2}
n<-1306
h1<-1.06*sd(problem_1_data)*n^(-1/5)
h2<-.9*min(c(IQR(problem_1_data)/1.34, sd(problem_1_data)))*n^(-1/5)
h0<-bw.nrd0(precip)

par(mfrow=c(2,2))
plot(density(problem_1_data)) #default Gaussian (h0)
plot(density(problem_1_data, bw=h1)) #Gaussian bandwidth h1
plot(density(problem_1_data, bw=h2)) #Gaussian bandwidth h2
plot(density(problem_1_data, kernel="cosine"))
par(mfrow=c(1,1))

print(c(h0, h1, h2))

d<-density(as.vector(problem_1_data))
xnew<-seq(0, 70, .01)
#for certain applications, it is helpful to create a function to return the estimates, which can be accomplished easily with approxfun
#fhat is a function returned by approxfun
fhat<-approxfun(d$x, d$y)

######
#reflection boundary technique
problem1_data<-as.vector(problem_1_data)
plot(density(problem1_data, bw=.6), xlim=c(-1,13), ylim=c(-.01,.30), main="")
abline(v=0)
grid()

xx<-c(problem1_data,-problem1_data)#distribution and the reflection
g<-density(xx, bw=h2)
a<-seq(0,70,.01)
ghat<-approx(g$x, g$y, xout=a) #numeric vectors giving the coordinates of pts to be interpolated, xout=set of numerical values where interpolation is to take place
fhat<-2*ghat$y  #density estimate along a (positive)

bw<-paste("Bandwidth = ", round(g$bw,5))
plot(a, fhat, type="l", xlim=c(-1,13), ylim=c(-.01,.30), main="", xlab=bw, ylab= "Density")
abline(v=0)
grid()



hist(problem_1_data, breaks=40,prob=TRUE, main="Histogram of Problem 1 Data")
lines(a,fhat, col="purple")

```

In your opinion, did you get a better estimate from what you created in Problem 1 or Problem 2?/
**I definitely got a better estimate by what I created in Problem 2, using the reflection technique.**


**You want to compute the optimal bandwidth before the "reflection."**


## Problem 3

For this problem, construct a Gibbs sampler to estimate the marginal densities of $x$ and $\theta$ given that their conditional densitites are 

$f(x|\theta) \sim$ Binomial$(n,\theta)$

$f(\theta|x) \sim$ Beta$(a + x, b + n - x)$

Evaluate Convergence by using trace plots and any other methods you like. 
What is your estimate for a reasonable burn-in period for this problem?

**I think the burn-in makes it worse. I don't think it's necessary**

```{r problem3}

num_iterations=10000
burn=300
n=100
theta=.25
a=3
b=8

chain_func<-function(){
vecx0<-numeric(num_iterations)
vecx1 <-numeric(num_iterations)
for(i in 1:num_iterations){
  
x0<-rbinom(1,n,theta)
x1<-rgamma(1,a+x0, b+n-x0)
vecx0[i]<-x0
vecx1[i]<-x1

}
return(data.frame(vecx0,vecx1))
}

chain_rep=replicate(20, chain_func())

#pts should be distributed around 25, since n=100 and theta=.25

#chain_rep[,1]$vecx0
#chain_rep[,1]$vecx1
vector1=cbind(chain_rep[,1]$vecx1,chain_rep[,2]$vecx1,chain_rep[,3]$vecx1,chain_rep[,4]$vecx1,
      chain_rep[,5]$vecx1,chain_rep[,6]$vecx1,chain_rep[,7]$vecx1,chain_rep[,8]$vecx1,
      chain_rep[,9]$vecx1,chain_rep[,10]$vecx1,chain_rep[,11]$vecx1,chain_rep[,12]$vecx1,
      chain_rep[,13]$vecx1,chain_rep[,14]$vecx1,chain_rep[,15]$vecx1,chain_rep[,16]$vecx1,
      chain_rep[,17]$vecx1,chain_rep[,18]$vecx1,chain_rep[,19]$vecx1,chain_rep[,20]$vecx1
      )


vector0=cbind(chain_rep[,1]$vecx0,chain_rep[,2]$vecx0,chain_rep[,3]$vecx0,chain_rep[,4]$vecx0,
      chain_rep[,5]$vecx0,chain_rep[,6]$vecx0,chain_rep[,7]$vecx0,chain_rep[,8]$vecx0,
      chain_rep[,9]$vecx0,chain_rep[,10]$vecx0,chain_rep[,11]$vecx0,chain_rep[,12]$vecx0,
      chain_rep[,13]$vecx0,chain_rep[,14]$vecx0,chain_rep[,15]$vecx0,chain_rep[,16]$vecx0,
      chain_rep[,17]$vecx0,chain_rep[,18]$vecx0,chain_rep[,19]$vecx0,chain_rep[,20]$vecx0
      )


cor(vector1[,1],vector0[,1])
cor(vector1[,2],vector0[,2])
cor(vector1[,3],vector0[,3])
cor(vector1[,4],vector0[,4])
cor(vector1[,5],vector0[,5])
cor(vector1[,6],vector0[,6])
cor(vector1[,7],vector0[,7])
cor(vector1[,8],vector0[,8])
cor(vector1[,9],vector0[,9])
cor(vector1[,10],vector0[,10])
cor(vector1[,11],vector0[,11])
cor(vector1[,12],vector0[,12])
plot(vector1,vector0)


M=as.vector(vector1[,1:20])
S=as.vector(vector0[,1:20])
X=cbind(M,S)
cor(X[,1],X[,2])
plot(X,cex=.5 )
burn=5000

b<-burn+1
x<-X[b:num_iterations,]
cor(x[,1],x[,2])
plot(x,cex=.5 )

colMeans(x)





```


## Problem 4

Use the Metropolis algorithm (symmetric proposal) to estimate the distribution of $X$ when we know \[f(x) \propto x(1-x^2)e^{-{x^2/2} + 4/{x^2}}.\] 
Use as your proposal functon \[x_{t+1} \sim q(x_t) = x_t + N(0,h^2) \] where $h$ is a scaling parameter. 
Only run the algorithm for a maximum of $50,000$ iterations. 

Does it converge for that number of iterations?

**No I don't think so.**

Create five "chains" and test for convergence using the Gelman-Rubin criterion.

Should you apply thinning to the output?
**I can, but I don't think it will help.**

```{r problem4}
n=length(problem_1_data)
h<-.9*min(c(IQR(problem_1_data)/1.34, sd(problem_1_data)))*n^(-1/5)

wrapper_ftn<-function(){
  
total_samples<-50000

target_dist <- function(x){
  return( x*(1-x^2)*(  exp((-x^2/2)*(4/x^2))  )  )
}

metro_samples <- numeric(total_samples + 1)
metro_samples[1] <- rnorm(1, 0, 1)

for (i in 1:total_samples){
  suggest<-rnorm(1, mean=metro_samples[i], sd=.001)
  unif_point <- runif(1)
  bound<-target_dist(suggest)/target_dist(metro_samples[i])
  if(unif_point < bound){
    metro_samples[i+1] <-suggest
  }else{
    metro_samples[i+1] <- metro_samples[i]
  }

}
return(metro_samples)
plot(metro_samples, type='l')
}

reps=replicate(20, expr={wrapper_ftn()})

plot(reps[,1], type='l')
plot(reps[,2], type='l')
plot(reps[,3], type='l')
plot(reps[,4], type='l')
plot(reps[,5], type='l')


#let me make a G-R statistic function
gelmanr<- function(psi){
  #psi<-as.matrix(psi)
  n<-ncol(psi)
  k<-nrow(psi)
  psi.means<-rowMeans(psi)    #find the chain mean
  B<-n*var(psi.means)         #what is the variance between the chains?
  psi.w<-apply(psi,1,"var")  #find variance of each row (or chain)
  W<-mean(psi.w)             #find the mean of the variance of each chain
  v.hat<-W*(n-1)/n + (B/n)   
  r.hat<-v.hat/W
  return(r.hat)
}

X<-matrix(0,nrow=20,ncol=50001)

for(i in 1:20)
  X[i,]<-reps[,i]

#psi<-t(apply(X,1,cumsum))
#for(i in 1:nrow(psi))
#  psi[i,]<-psi[i,]/(1:ncol(psi))
#print(gelmanr(X))

gelmanr(X)

```

## Problem 5
Note: This problem requires the use of the data set problem5.Rdata.

With the data sets $X$ and $Y$ in $R^2$ in the problem5.Rdata file, use the nearest neighbor test to test the null hypothesis that the sets $X$ and $Y$ are generated from the same distribution. Use up to the $6^{th}$ nearest neighbor.

```{r problem5}
load("C:/Users/Owner/Downloads/problem5.Rdata")
length(X)
length(Y)
head(X)
head(Y)
#glimpse(X)
#glimpse(Y)

dim(X)
dim(Y)

#ftn to compute the avg fraction of the r nearest neighbors that are from the same set for each point in the input sets. 
compute_nn_fraction <- function(ordered_dist_matrix, set_x_prime, set_y_prime, r){
  s = length(set_x_prime) + length(set_y_prime)
  score <- 0
  for(some_row in set_x_prime){
    score <- score + sum(ordered_dist_matrix[some_row, 2:(r+1)] %in% set_x_prime) / r
  }
  for(some_row in set_y_prime){
    score <- score + sum(ordered_dist_matrix[some_row, 2:(r+1)] %in% set_y_prime) / r
  }
  return(score / s)
}
```

```{r}
#ftn to compute the distance matrix between the input sets X and Y, compute the nearest neighbor statistic between those two sets, then run a nearest neighbor permutation test and return the emperical p-value for that test.
compute_epvalue_nn_test <- function(set_x, set_y, r, num_replicates = 1000){
  n = dim(set_x)[1]
  m = dim(set_y)[1]
  s = n + m
  
  set_z <- rbind(set_x, set_y)
  pairwise_dist <- as.matrix(dist(set_z, diag = TRUE, upper = TRUE))
  nearest_neighbor_matrix <- matrix(rep(NA, s * s), s, s)
  # Create a nearest neighbor matrix where each row is ordered by the indices of the 
  # nearest neighbors
  for(index in 1:s){
    nearest_neighbor_matrix[index, ] <- order(pairwise_dist[index, ])
  }
  # Because we are doing a permutation test, we only need to know the indices of the
  # X and Y values in the set Z, not their actual values. That's why we sorted the
  # distance matrix into a nearest neighbor matrix.
  orig_stat <- compute_nn_fraction(nearest_neighbor_matrix, 1:n, (n + 1):s, r)
  dist_stats <- replicate(num_replicates, expr = {
    permutation <- sample(1:s, s, replace = FALSE)
    set_x_prime <- permutation[1:n]
    set_y_prime <- permutation[(n + 1):s]
    compute_nn_fraction(nearest_neighbor_matrix, set_x_prime, set_y_prime, r)
  })
  
  epvalue <- (1 + sum(dist_stats > orig_stat)) / (num_replicates + 1)
  return(epvalue)
}
```

```{r}
require(MASS)
#data and run hypothesis test
set_x <- X
set_y <- Y
compute_epvalue_nn_test(set_x, set_y, 6)
```



## Problem 6

Use the percentile bootstrap method for generating a confidence interval around the mean of the data set defined below.

```{r problem6}
boot_data <- c(0.1, 1.32, 2, 3.4, 1, 2.63, 0.3, 0.68, 5, 2.65, 3.1, 0.66)

resamples <- 10000
bootstrap_means <- replicate(resamples, mean(sample(x = boot_data, size = length(boot_data), replace = TRUE ) ) )
bootstrap_means <- bootstrap_means[order(bootstrap_means)]
hist(bootstrap_means, breaks = 50, prob = TRUE)
confidence <- 0.05
lower_bound <- round(confidence/2*resamples, 0)
upper_bound <- round(resamples - confidence/2*resamples, 0)
conf_interval <- c(bootstrap_means[lower_bound], bootstrap_means[upper_bound])
conf_interval

```

## Problem 7

Using any Monte Carlo method of your choice, evaluate the integral $\int_0^5 (1+x^2)e^{-x^2}dx$. Generate some confidence intervals for that estimate.

```{r problem7}
#let me just try using uniform samples
my_func <- function(x){
  return((1+x^2)*exp(-x^2))
}

mc_estimate <- function(func_to_integrate, lower_bound, upper_bound, num_samples){
  unif_samples <- runif(num_samples, lower_bound, upper_bound)
  return( mean(func_to_integrate(unif_samples))*5 )
}

rep=replicate(1000, {mc_estimate(my_func, 0, 5, 1000)})
mean(rep)

se_estimates=1.96*sd(rep)/(sqrt(1000/2))

CI=c(LowerBound=mean(rep)-se_estimates,UpperBound=mean(rep)+se_estimates)
CI

```


## Problem 8

Find the emperical Type-I error rate of the t-test when the underlying data is generated from a Beta$(3,5)$ distribution.

```{r, problem8}
sig_level=.05
num_samples=100
true_mean_beta=3/8

test_the_t_test=function(num_samples, sig_level, true_mean_beta){
beta_samples=rbeta(num_samples, 3,5)
t.test(beta_samples, alternative="two.sided", mu=true_mean_beta)$p.value < sig_level
}

type_1_error=replicate(1000,expr={test_the_t_test(num_samples, sig_level, true_mean_beta)})
error_rate=mean(type_1_error)
standard_error=sqrt(error_rate*(1-error_rate)/num_samples)
error_rate
standard_error

```
Let me see how this changes as the number of samples change with .05 significance.

```{r}
sig_level=.05
num_samples=1000
true_mean=3/8

test_the_t_test=function(num_samples, sig_level, true_mean){
beta_samples=rbeta(num_samples, 3,5)
t.test(beta_samples, alternative="two.sided", mu=true_mean)$p.value < sig_level
}

type_1_error=replicate(10000,expr={test_the_t_test(num_samples, sig_level, true_mean)})
error_rate=mean(type_1_error)
standard_error=sqrt(error_rate*(1-error_rate)/num_samples)
error_rate
standard_error

error_estimates <- lapply(2:100, FUN = function(x) {
  type_1_error <- replicate(1000, expr = {test_the_t_test(x, sig_level, true_mean)})
error_rate <- mean(type_1_error)
standard_error <- sqrt(error_rate* (1 - error_rate) / num_samples )
  return(data.frame(error_rate, standard_error))
})
```
**Looks like it's about .05.**
```{r}


plot(2:100, unlist(lapply(error_estimates, function(x) return(x[[1]]))), main = "T Test Type I Error for Beta Distribution, alpha = 0.05",xlab = "Number of Samples from Beta Distribution", ylab = "Type I Error Rate")
```

## Problem 9

What was the most interesting thing you learned in this course and why?

**I knew I could sample and do these things in R but I didn't understand why I should or would want to do them. So learning the why of everything was the most interesting thing that I learned. Particularly, why you should use one sampling method over another, ie inverse transform sampling instead of Gibbs sampling, in certain scenarios. I also enjoyed learning how to create my own histogram and build out my own functions. The more I learn R and other languages, I'm realizing how some of these baked in functions are not always necessary! I definitely am more confident in my R skills after this course - thanks Shane!** 



